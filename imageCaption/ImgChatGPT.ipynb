{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsYKc1zGz2VR"
      },
      "outputs": [],
      "source": [
        "#####Step 1: Import the necessary libraries and download the dataset.\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import pickle\n",
        "import os\n",
        "import re\n",
        "\n",
        "# Download and extract the Flickr8k dataset: https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\n",
        "# Download the captions file: https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip\n",
        "\n",
        "# Set the paths to your dataset and captions\n",
        "image_folder = 'Flickr8k_Dataset'\n",
        "captions_file = 'Flickr8k.token.txt'\n",
        "\n",
        "\n",
        "######Step 2: Preprocess the dataset, including image feature extraction and text tokenization.\n",
        "\n",
        "# Image feature extraction using InceptionV3\n",
        "def preprocess_images(image_path):\n",
        "    model = InceptionV3(include_top=False, weights='imagenet')\n",
        "    model = Model(inputs=model.input, outputs=model.layers[-1].output)\n",
        "\n",
        "    image = keras.preprocessing.image.load_img(image_path, target_size=(299, 299))\n",
        "    image = keras.preprocessing.image.img_to_array(image)\n",
        "    image = np.expand_dims(image, axis=0)\n",
        "    image = keras.applications.inception_v3.preprocess_input(image)\n",
        "\n",
        "    features = model.predict(image)\n",
        "    return features\n",
        "\n",
        "# Tokenize captions\n",
        "def tokenize_captions(captions):\n",
        "    tokenizer = Tokenizer(num_words=5000, oov_token=\"<OOV>\")\n",
        "    tokenizer.fit_on_texts(captions)\n",
        "    return tokenizer\n",
        "\n",
        "# Load and preprocess captions\n",
        "with open(captions_file, 'r') as file:\n",
        "    captions = file.readlines()\n",
        "\n",
        "# Initialize a list to store preprocessed captions\n",
        "preprocessed_captions = []\n",
        "\n",
        "for caption in captions:\n",
        "    # Preprocess one caption at a time\n",
        "    # Convert to lowercase\n",
        "    caption = caption.lower()\n",
        "\n",
        "    # Remove digits, special characters, and anything that is not a letter\n",
        "    caption = re.sub(r'[^a-zA-Z]', ' ', caption)\n",
        "\n",
        "    # Remove additional spaces\n",
        "    caption = re.sub(r'\\s+', ' ', caption).strip()\n",
        "\n",
        "    # Add start and end tags to the caption\n",
        "    caption = '<start> ' + caption + ' <end>'\n",
        "\n",
        "    preprocessed_captions.append(caption)\n",
        "\n",
        "tokenizer = tokenize_captions(preprocessed_captions)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "sequences = tokenizer.texts_to_sequences(preprocessed_captions)\n",
        "max_sequence_length = max(len(sequence) for sequence in sequences)\n",
        "sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#####Step 3: Create an image captioning model using LSTM and train it.\n",
        "# Image captions model\n",
        "image_model = keras.Sequential([\n",
        "    keras.layers.Input(shape=(2048,)),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(256, activation='relu'),\n",
        "])\n",
        "\n",
        "caption_model = keras.Sequential([\n",
        "    keras.layers.Input(shape=(max_sequence_length,)),\n",
        "    keras.layers.Embedding(input_dim=vocab_size, output_dim=256, mask_zero=True),\n",
        "    keras.layers.LSTM(256),\n",
        "    keras.layers.Dense(256, activation='relu'),\n",
        "])\n",
        "\n",
        "combined_model = keras.layers.Concatenate()([image_model.output, caption_model.output])\n",
        "x = keras.layers.Dense(256, activation='relu')(combined_model)\n",
        "outputs = keras.layers.Dense(vocab_size, activation='softmax')(x)\n",
        "\n",
        "model = keras.Model(inputs=[image_model.input, caption_model.input], outputs=outputs)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=RMSprop(), metrics=['accuracy'])\n",
        "\n",
        "# Train the model using your dataset\n",
        "\n",
        "\n",
        "#######Step 4: Generate captions for images using the trained model\n",
        "# Generate captions for images\n",
        "def generate_caption(image_path):\n",
        "    image = preprocess_images(image_path)\n",
        "    image_features = image_model.predict(image)\n",
        "\n",
        "    caption = ['<start>']\n",
        "    for _ in range(max_sequence_length):\n",
        "        caption_sequence = tokenizer.texts_to_sequences([caption])[0]\n",
        "        caption_sequence = pad_sequences([caption_sequence], maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "        next_word_prob = model.predict([image_features, caption_sequence])[0]\n",
        "        next_word = tokenizer.index_word[np.argmax(next_word_prob)]\n",
        "\n",
        "        if next_word == '<end>':\n",
        "            break\n",
        "        caption.append(next_word)\n",
        "\n",
        "    return ' '.join(caption[1:-1])\n",
        "\n",
        "# Example usage\n",
        "image_path = 'example.jpg'\n",
        "caption = generate_caption(image_path)\n",
        "print(caption)\n"
      ]
    }
  ]
}